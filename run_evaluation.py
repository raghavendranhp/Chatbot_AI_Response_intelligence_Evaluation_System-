import os
import sys
import pandas as pd
from src.orchestrator import CruiseOrchestrator
from evaluation.engine import EvaluationEngine

# --- Configuration ---
GROUND_TRUTH_PATH = "data/ground_truth/Egypt_Cruise_GroundTruth_Dataset.csv"
SAMPLE_QUERIES = [
    # 1. Simple Fact (Should pass easily)
    "What is the price of the Nile Explorer?",
    
    # 2. Complex Constraint (Needs logic)
    "Find me a luxury cruise under $1000 for 5 days.",
    
    # 3. Comparison (Needs structure)
    "Compare the Pharaoh Classic and the Royal Nile.",
    
    # 4. Hallucination Trap (Fake Cruise ID)
    "Tell me about the Galaxy Voyager (CRZ999).",
    
    # 5. Hallucination Trap (Fake Price Constraint)
    "I want a 7-day luxury cruise for $50.",
    
    # 6. Out of Scope (Should be handled gracefully)
    "Do you have cruises to the Caribbean?",
    
    # 7. Ambiguous/Broad
    "Tell me about Egypt cruises.",
    
    # 8. Specific Itinerary Detail
    "Which cruise visits Kom Ombo?"
]

def main():
    print("="*60)
    print("üöÄ INITIALIZING AI RESPONSE INTELLIGENCE SYSTEM")
    print("="*60)

    # 1. Initialize the Chatbot (The System Under Test)
    # We use your existing Orchestrator
    try:
        chatbot = CruiseOrchestrator()
        print("[INFO] Chatbot Orchestrator loaded successfully.")
    except Exception as e:
        print(f"[ERROR] Failed to load Chatbot: {e}")
        sys.exit(1)

    # 2. Initialize the Evaluator (The Judge)
    # We pass the chatbot's LLM function so the Judge uses the same brain
    # (Assuming chatbot.llm_client.generate_text exists or similar)
    try:
        # Note: Adjust 'chatbot.generate_text' to match your actual LLM method name
        # If your Orchestrator doesn't expose the raw LLM, we might need a wrapper.
        # For now, we assume the orchestrator has a method we can use.
        evaluator = EvaluationEngine(
            llm_callable=chatbot.llm_client.generate_response, 
            ground_truth_path=GROUND_TRUTH_PATH
        )
        print("[INFO] Evaluation Engine loaded successfully.")
    except Exception as e:
        print(f"[ERROR] Failed to load Evaluator: {e}")
        # Fallback for demo if LLM client isn't reachable
        sys.exit(1)

    results = []

    print("\n" + "="*60)
    print("üß™ STARTING BATCH EVALUATION")
    print("="*60)

    for i, query in enumerate(SAMPLE_QUERIES, 1):
        print(f"\nüîπ TEST CASE {i}: '{query}'")
        
        # A. Get Chatbot Response
        # We assume answer_query returns (response_text, retrieved_context)
        # If it only returns text, we pass empty context to the evaluator
        try:
            response = chatbot.process_query(query)
            
            # Extract text and context if your orchestrator returns a dict/object
            # Adjust these keys based on your actual src/orchestrator.py return format
            if isinstance(response, dict):
                answer_text = response.get("answer", "")
                context_text = response.get("context", "")
            else:
                answer_text = str(response)
                context_text = "" # Context missing, Evaluation will be partial
                
            print(f"   ü§ñ AI Answer: {answer_text[:100]}...")
            
        except Exception as e:
            print(f"   ‚ùå Chatbot Failed: {e}")
            continue

        # B. Evaluate Response
        try:
            eval_report = evaluator.evaluate(query, answer_text, context_text)
            
            # Print Mini Report
            score = eval_report['overall_score']
            status = eval_report['status']
            flags = eval_report['flags']
            
            print(f"   ‚öñÔ∏è  Score: {score}/1.0  [{status}]")
            if flags:
                print(f"   üö© Flags: {flags}")
            
            results.append(eval_report)
            
        except Exception as e:
            print(f"   ‚ùå Evaluation Failed: {e}")

    # 3. Final Summary
    print("\n" + "="*60)
    print("üìä FINAL DEMO REPORT")
    print("="*60)
    
    df = pd.DataFrame(results)
    if not df.empty:
        print(f"Total Queries: {len(df)}")
        print(f"Average Score: {df['overall_score'].mean():.2f}")
        print(f"Pass Rate: {len(df[df['status'] == 'PASS']) / len(df) * 100:.1f}%")
        
        # Check for Feedback Signals
        print("\nüîç SYSTEM TUNING SIGNALS:")
        report = evaluator.feedback.generate_report()
        if isinstance(report, dict) and "tuning_recommendations" in report:
            for rec in report["tuning_recommendations"]:
                print(f"   - {rec}")
    else:
        print("No results generated.")

if __name__ == "__main__":
    main()